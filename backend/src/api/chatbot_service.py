import asyncio
from src.chatbot.core_initializer import CoreInitializer
from src.config.config_url import DOCS_URL
from src.chatbot.graph_initializer import GraphInitializer
from src.chatbot.chat_initializer import ChatInitializer

class ChatbotService:
    def __init__(self, docs_path="./docs", web_paths=DOCS_URL):
        """
        Initializes the chatbot system and starts a conversation.

        This function sets up the core components of the chatbot, creates the chatbot graph,
        and starts a conversation using the initialized components.
        """
        # Initialize core components (LLM, embeddings, etc.)
        core = CoreInitializer(docs_path=docs_path, web_paths=web_paths)
        core.initialize()

        # Retrieve initialized managers
        model_manager = core.model_manager
        embedding_manager = core.embedding_manager

        # Create and compile the chatbot graph
        chatbot_graph = GraphInitializer(
            model_manager=model_manager, embedding_manager=embedding_manager
        )
        chatbot_graph.build_graph()

        # Configure and start the chatbot conversation
        config = {"configurable": {"thread_id": "1"}}
        self.chatbot = ChatInitializer(chatbot_graph=chatbot_graph, config=config)
        
    async def generate_response(self, question):
        """
        Genera la respuesta completa del chatbot sin streaming.
        """
        response = await self.chatbot._chatbot_graph.graph.ainvoke(
            {"question": question}, self.chatbot._config
        )
        return response["answer"]
    
    async def generate_response_stream(self, question):
        """
        Streams the chatbot's answer in chunks (token by token).
        Yields text chunks as they are generated by the model.
        """
        async for event in self.chatbot._chatbot_graph.graph.astream_events(
            {"question": question}, self.chatbot._config
        ):
            if (
                event["event"] == "on_chat_model_stream"
                and event["metadata"].get("langgraph_node", "") == "generate_answer"
            ):
                data = event["data"]
                content = data["chunk"].content
                if content:
                    yield f"data: {content}\n\n"  # Formato SSE: "data: <message>\n\n"
